{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pickle, math, os\n",
    "import numpy as np\n",
    "from os.path import join, dirname, expanduser, exists\n",
    "lab_dnn = torch.load(\"lab_dnn.pt\")\n",
    "out = torch.load(\"out.pt\")\n",
    "lab_conc = np.load(\"lab_conc.npy\")\n",
    "mapping = np.load(\"mapping.npy\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "conf_dir = join(expanduser(\"~\"), \"UPM\", \"conf\")\n",
    "phones_filepath = join(conf_dir, \"articulatory_features\", \"phone_attributes_filtered.txt\")\n",
    "feats_filepath = join(conf_dir, \"articulatory_features\", \"feature_vectors.txt\")\n",
    "extensions_filepath = join(conf_dir, \"articulatory_features\", \"extensions_filtered.txt\")\n",
    "curr_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_chunks = np.load(\"lab_chunks.npy\", allow_pickle=True)\n",
    "fea_chunks = np.load(\"fea_chunks.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_conc, lab_conc, _, _ = _concatenate_features_and_labels(fea_chunks, lab_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336106, 69)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336106,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_conc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_conc = np.expand_dims(lab_conc,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_mapping = mapping[lab_conc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336106, 51)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_mapping.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_conc = np.concatenate((lab_mapping, lab_conc), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO convert phone to phones.txt when reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_valid_extensions(extensions_filepath, phones_idx_dict, feat_idx_dict):\n",
    "    vowel_extensions = []\n",
    "    consonant_extensions = []\n",
    "    with open(extensions_filepath, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "        \n",
    "    for line in lines:\n",
    "        is_valid = False\n",
    "        entry = line.split()\n",
    "        extension = entry[0]\n",
    "        # Check if extension occurs in any of the phones\n",
    "        for phone in phones_idx_dict:\n",
    "            if extension in phone:\n",
    "                is_valid = True\n",
    "        \n",
    "        if is_valid:\n",
    "            extension_atts = [feat_idx_dict[x] for x in entry[1:]]\n",
    "            if extension in [\":\"]: # This extension seems to only apply to vowels\n",
    "                vowel_extensions.append([extension] + extension_atts)\n",
    "            else:\n",
    "                consonant_extensions.append([extension] + extension_atts)\n",
    "    return vowel_extensions, consonant_extensions\n",
    "\n",
    "def get_phone_idx_dict(phones_txt_filepath):\n",
    "    with open(phones_txt_filepath, \"r\") as f:\n",
    "        phone_to_idx_dict = {}\n",
    "        for line in f:\n",
    "            entry = line.split()\n",
    "            phone = entry[0]\n",
    "            idx = int(entry[1])\n",
    "            phone_to_idx_dict[phone] = idx\n",
    "    return phone_to_idx_dict\n",
    "\n",
    "def mapping_to_labels(mapping, lab_dnn):\n",
    "    lab_detached = lab_dnn.detach().cpu().numpy()\n",
    "    outputs = np.zeros(lab_detached.shape[0])\n",
    "    for lab_idx, lab in enumerate(lab_detached):\n",
    "        if sum(lab) == 0:\n",
    "            outputs[lab_idx] = 1\n",
    "        else:\n",
    "            for mapping_idx, map_val in enumerate(mapping):\n",
    "                if np.array_equal(map_val, lab):\n",
    "                    outputs[lab_idx] = mapping_idx\n",
    "    return outputs\n",
    "\n",
    "def get_feat_idx(feats_filepath):\n",
    "    with open(feats_filepath, \"r\") as f:\n",
    "        header = f.readline()\n",
    "    features = header.split()[1:]\n",
    "    feat_idx_dict = {}\n",
    "    for idx, feat in enumerate(features):\n",
    "        feat_idx_dict[feat] = idx\n",
    "    return feat_idx_dict\n",
    "        \n",
    "def add_extensions(extensions, phones, curr_phone, phone_to_idx_dict, remove_val):\n",
    "    # Each extension is of the format [\":\", 3, 2]\n",
    "    for extension in extensions:\n",
    "        # Only add new attributes\n",
    "        atts_to_add = [x for x in extension[1:] if x not in curr_phone[1:]]\n",
    "        if len(atts_to_add) > 0:\n",
    "            ext_phone = curr_phone[0] + extension[0]\n",
    "            if ext_phone in phone_to_idx_dict:\n",
    "                ext_phone_idx = phone_to_idx_dict[ext_phone]\n",
    "            else:\n",
    "                ext_phone_idx = 0\n",
    "            ext_feat = [ext_phone_idx] + curr_phone[1:] + atts_to_add\n",
    "            ext_feat.remove(remove_val)\n",
    "            phones.append(ext_feat)\n",
    "    return phones\n",
    "    \n",
    "def get_phones(phones_filepath, feat_idx_dict, phone_to_idx_dict, ve, ce, include_extensions=True):\n",
    "    # ve, ce are vowel and consonant extensions to add\n",
    "    with open(phones_filepath, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "    vowel_phones = []\n",
    "    consonant_phones = []\n",
    "    vowel_idx = feat_idx_dict[\"vowel\"]\n",
    "    consonant_idx = feat_idx_dict[\"consonant\"]  \n",
    "    \n",
    "    for line in lines:\n",
    "        # Entry = [\"a\", \"front\", \"close\", etc.]\n",
    "        entry = line.split()\n",
    "        is_vowel = False\n",
    "        # Convert to indices for speed\n",
    "        for idx, att in enumerate(entry):\n",
    "            if att == \"vowel\":\n",
    "                remove_idx = idx\n",
    "                is_vowel = True\n",
    "            elif att == \"consonant\":\n",
    "                remove_idx = idx\n",
    "                \n",
    "            if att in feat_idx_dict:\n",
    "                entry[idx] = feat_idx_dict[att]\n",
    "                \n",
    "        if is_vowel and include_extensions:\n",
    "            vowel_phones = add_extensions(ve, vowel_phones, entry, phone_to_idx_dict, vowel_idx)\n",
    "        elif include_extensions:\n",
    "            consonant_phones = add_extensions(ce, consonant_phones, entry, phone_to_idx_dict, consonant_idx)\n",
    " \n",
    "        # Entry[0] is the phone e.g. \"a\"\n",
    "        phone = entry[0]\n",
    "        # Special case for J\\: (rare incidence of consonant + long in Turkish)\n",
    "        if phone == \"J\\\\\" and \"J\\\\:\" in phone_to_idx_dict:\n",
    "            long_idx = feat_idx_dict[\"long\"]\n",
    "            extra_phone_idx = phone_to_idx_dict[\"J\\\\:\"]\n",
    "            extra_feat = [extra_phone_idx] + entry[1:] + [long_idx]\n",
    "            extra_feat.remove(consonant_idx)\n",
    "            consonant_phones.append(extra_feat)\n",
    "            \n",
    "        if phone in phone_to_idx_dict:\n",
    "            entry[0] = phone_to_idx_dict[phone]\n",
    "        else:\n",
    "            # Set unknown phones to <eps> ?\n",
    "            entry[0] = 0\n",
    "        del entry[remove_idx]\n",
    "        if is_vowel:\n",
    "            vowel_phones.append(entry)\n",
    "        else:\n",
    "            consonant_phones.append(entry)\n",
    "\n",
    "    return vowel_phones, consonant_phones\n",
    "\n",
    "# Need to initially split between vowels and consonants before further on features\n",
    "def initial_split(phones, feat_idx_dict):\n",
    "    vowel_phones = []\n",
    "    consonant_phones = []\n",
    "    vowel_idx = feat_idx_dict[\"vowel\"]\n",
    "    consonant_idx = feat_idx_dict[\"consonant\"]\n",
    "    for phone in phones:\n",
    "        if vowel_idx in phone:\n",
    "            phone.remove(vowel_idx)\n",
    "            vowel_phones.append(np.asarray(phone))\n",
    "\n",
    "        elif consonant_idx in phone:\n",
    "            phone.remove(consonant_idx)\n",
    "            consonant_phones.append(np.asarray(phone))\n",
    "                    \n",
    "    vowel_phones = np.asarray(vowel_phones)\n",
    "    consonant_phones = np.asarray(consonant_phones)\n",
    "    return vowel_phones, consonant_phones\n",
    "\n",
    "def save_variables(feat_idx_dict, vp, cp, vp_idx, cp_idx, filename=None, save_dir=None):\n",
    "    if save_dir is None:\n",
    "        save_dir = os.getcwd()\n",
    "    if filename is None:\n",
    "        save_path = join(save_dir, \"feat_vars.pkl\")\n",
    "    else:\n",
    "        save_path = join(save_dir, filename + \".pkl\")\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump([feat_idx_dict, vp, cp, vp_idx, cp_idx], f)\n",
    "\n",
    "feats_filepath = join(conf_dir, \"articulatory_features\", \"feature_vectors.txt\")\n",
    "phones_filepath = join(conf_dir, \"articulatory_features\", \"phone_attributes_filtered.txt\")\n",
    "phones_list_filepath = join(dirname(os.getcwd()), \"notes\", \"phones.txt\")\n",
    "phone_idx_dict = get_phone_idx_dict(phones_list_filepath)\n",
    "feat_idx_dict = get_feat_idx(feats_filepath)\n",
    "ve, ce = filter_valid_extensions(extensions_filepath, phone_idx_dict, feat_idx_dict)\n",
    "\n",
    "\n",
    "vp, cp = get_phones(phones_filepath, feat_idx_dict, phone_idx_dict, ve, ce)\n",
    "\n",
    "# Get tensor of consonants e.g [0 3 4 5 112 1] - avoids excessive converting to and from tensors\n",
    "cp_idx = [x[0] for x in cp]\n",
    "cp_idx = torch.from_numpy(np.asarray(cp_idx))\n",
    "\n",
    "# Get tensor of consonants e.g [0 3 4 5 112 1] - avoids excessive converting to and from tensors\n",
    "vp_idx = [x[0] for x in vp]\n",
    "vp_idx = torch.from_numpy(np.asarray(vp_idx))\n",
    "\n",
    "save_variables(feat_idx_dict, vp, cp, vp_idx, cp_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['_<', 23], ['_>', 17], ['_~', 30], ['_h', 5], ['_j', 36]]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\Paul\\UPM\\pytorch-kaldi\\universal_phones.txt\n"
     ]
    }
   ],
   "source": [
    "def build_universal_phonemap(phones_filepath, ve, ce, write_filepath=None):\n",
    "    with open(phones_filepath, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "    \n",
    "    write_lines = [\"<eps> 0\", \"sil 1\"]\n",
    "    next_num = 2\n",
    "    \n",
    "    for line in lines:\n",
    "        curr_phone = line.split()\n",
    "        phone = curr_phone[0]\n",
    "        write_lines.append(\"{} {}\".format(phone, next_num))\n",
    "        next_num += 1\n",
    "        if \"vowel\" in curr_phone:\n",
    "            for ext_list in ve:\n",
    "                ext = ext_list[0]\n",
    "                phone_ext = phone + ext\n",
    "                write_lines.append(\"{} {}\".format(phone_ext, next_num))\n",
    "                next_num += 1\n",
    "                \n",
    "        elif \"consonant\" in curr_phone:\n",
    "            for ext_list in ce:\n",
    "                ext = ext_list[0]\n",
    "                phone_ext = phone + ext\n",
    "                write_lines.append(\"{} {}\".format(phone_ext, next_num))\n",
    "                next_num += 1\n",
    "                \n",
    "    # Special case of \"long\" applying to consonant\n",
    "    write_lines.append(\"J\\\\: {}\".format(next_num))\n",
    "                \n",
    "    if write_filepath is None:\n",
    "        write_filepath = join(os.getcwd(), \"universal_phones.txt\")\n",
    "        \n",
    "    with open(write_filepath, \"w\") as f:\n",
    "        print(\"Writing to {}\".format(write_filepath))\n",
    "        for line in write_lines:\n",
    "            f.write(line + \"\\n\")\n",
    "            \n",
    "build_universal_phonemap(phones_filepath, ve, ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phones(phones_filepath, feat_idx_dict, phone_to_idx_dict, ve, ce, include_extensions=True):\n",
    "    # ve, ce are vowel and consonant extensions to add\n",
    "    with open(phones_filepath, \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "    vowel_phones = []\n",
    "    consonant_phones = []\n",
    "    vowel_idx = feat_idx_dict[\"vowel\"]\n",
    "    consonant_idx = feat_idx_dict[\"consonant\"]  \n",
    "    \n",
    "    for line in lines:\n",
    "        # Entry = [\"a\", \"front\", \"close\", etc.]\n",
    "        entry = line.split()\n",
    "        is_vowel = False\n",
    "        # Convert to indices for speed\n",
    "        for idx, att in enumerate(entry):\n",
    "            if att == \"vowel\":\n",
    "                remove_idx = idx\n",
    "                is_vowel = True\n",
    "            elif att == \"consonant\":\n",
    "                remove_idx = idx\n",
    "                \n",
    "            if att in feat_idx_dict:\n",
    "                entry[idx] = feat_idx_dict[att]\n",
    "                \n",
    "        if is_vowel and include_extensions:\n",
    "            vowel_phones = add_extensions(ve, vowel_phones, entry, phone_to_idx_dict, vowel_idx)\n",
    "        elif include_extensions:\n",
    "            consonant_phones = add_extensions(ce, consonant_phones, entry, phone_to_idx_dict, consonant_idx)\n",
    " \n",
    "        # Entry[0] is the phone e.g. \"a\"\n",
    "        phone = entry[0]\n",
    "        # Special case for J\\: (rare incidence of consonant + long in Turkish)\n",
    "        if phone == \"J\\\\\" and \"J\\\\:\" in phone_to_idx_dict:\n",
    "            long_idx = feat_idx_dict[\"long\"]\n",
    "            extra_phone_idx = phone_to_idx_dict[\"J\\\\:\"]\n",
    "            extra_feat = [extra_phone_idx] + entry[1:] + [long_idx]\n",
    "            extra_feat.remove(consonant_idx)\n",
    "            consonant_phones.append(extra_feat)\n",
    "            \n",
    "        if phone in phone_to_idx_dict:\n",
    "            entry[0] = phone_to_idx_dict[phone]\n",
    "        else:\n",
    "            # Set unknown phones to <eps> ?\n",
    "            entry[0] = 0\n",
    "        del entry[remove_idx]\n",
    "        if is_vowel:\n",
    "            vowel_phones.append(entry)\n",
    "        else:\n",
    "            consonant_phones.append(entry)\n",
    "\n",
    "    return vowel_phones, consonant_phones\n",
    "\n",
    "\n",
    "\n",
    "feats_filepath = join(conf_dir, \"articulatory_features\", \"feature_vectors.txt\")\n",
    "phones_list_filepath = join(os.getcwd(), \"universal_phones.txt\")\n",
    "phones_filepath = join(conf_dir, \"articulatory_features\", \"phone_attributes_filtered.txt\")\n",
    "phone_idx_dict = get_phone_idx_dict(phones_list_filepath)\n",
    "feat_idx_dict = get_feat_idx(feats_filepath)\n",
    "ve, ce = filter_valid_extensions(extensions_filepath, phone_idx_dict, feat_idx_dict)\n",
    "\n",
    "\n",
    "vp, cp = get_phones(phones_filepath, feat_idx_dict, phone_idx_dict, ve, ce)\n",
    "\n",
    "# Get tensor of consonants e.g [0 3 4 5 112 1] - avoids excessive converting to and from tensors\n",
    "cp_idx = [x[0] for x in cp]\n",
    "cp_idx = torch.from_numpy(np.asarray(cp_idx))\n",
    "\n",
    "# Get tensor of consonants e.g [0 3 4 5 112 1] - avoids excessive converting to and from tensors\n",
    "vp_idx = [x[0] for x in vp]\n",
    "vp_idx = torch.from_numpy(np.asarray(vp_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_ne, cp_ne = get_phones(phones_filepath, feat_idx_dict, phone_idx_dict, ve, ce, include_extensions=False)\n",
    "\n",
    "cp_idx_ne = [x[0] for x in cp_ne]\n",
    "cp_idx_ne = torch.from_numpy(np.asarray(cp_idx_ne))\n",
    "\n",
    "# Get tensor of consonants e.g [0 3 4 5 112 1] - avoids excessive converting to and from tensors\n",
    "vp_idx_ne = [x[0] for x in vp_ne]\n",
    "vp_idx_ne = torch.from_numpy(np.asarray(vp_idx_ne))\n",
    "\n",
    "save_variables(feat_idx_dict, vp_ne, cp_ne, vp_idx_ne, cp_idx_ne, filename=\"feat_vars_ne\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[39, 48, 7, 43, 24, 23],\n",
       " [0, 48, 7, 43, 24, 17],\n",
       " [0, 48, 7, 43, 24, 30],\n",
       " [0, 48, 7, 43, 24, 5],\n",
       " [40, 48, 7, 43, 24, 36],\n",
       " [38, 48, 7, 43, 24],\n",
       " [0, 49, 36, 43, 16, 23],\n",
       " [0, 49, 36, 43, 16, 17],\n",
       " [0, 49, 36, 43, 16, 30],\n",
       " [0, 49, 36, 43, 16, 5],\n",
       " [42, 49, 36, 43, 16],\n",
       " [45, 48, 1, 39, 14, 23],\n",
       " [0, 48, 1, 39, 14, 17],\n",
       " [0, 48, 1, 39, 14, 30],\n",
       " [0, 48, 1, 39, 14, 5],\n",
       " [46, 48, 1, 39, 14, 36],\n",
       " [43, 48, 1, 39, 14],\n",
       " [0, 48, 41, 39, 14, 23],\n",
       " [0, 48, 41, 39, 14, 17],\n",
       " [0, 48, 41, 39, 14, 30],\n",
       " [0, 48, 41, 39, 14, 5],\n",
       " [0, 48, 41, 39, 14, 36],\n",
       " [48, 48, 41, 39, 14],\n",
       " [0, 49, 27, 20, 24, 23],\n",
       " [0, 49, 27, 20, 24, 17],\n",
       " [0, 49, 27, 20, 24, 30],\n",
       " [0, 49, 27, 20, 24, 5],\n",
       " [55, 49, 27, 20, 24, 36],\n",
       " [54, 49, 27, 20, 24],\n",
       " [0, 49, 27, 20, 7, 39, 24, 23],\n",
       " [0, 49, 27, 20, 7, 39, 24, 17],\n",
       " [0, 49, 27, 20, 7, 39, 24, 30],\n",
       " [0, 49, 27, 20, 7, 39, 24, 5],\n",
       " [0, 49, 27, 20, 7, 39, 24, 36],\n",
       " [0, 49, 27, 20, 7, 39, 24],\n",
       " [57, 48, 47, 39, 16, 23],\n",
       " [0, 48, 47, 39, 16, 17],\n",
       " [0, 48, 47, 39, 16, 30],\n",
       " [0, 48, 47, 39, 16, 5],\n",
       " [58, 48, 47, 39, 16, 36],\n",
       " [56, 48, 47, 39, 16],\n",
       " [0, 49, 22, 20, 23],\n",
       " [0, 49, 22, 20, 17],\n",
       " [0, 49, 22, 20, 30],\n",
       " [0, 49, 22, 20, 5],\n",
       " [0, 49, 22, 20, 36],\n",
       " [60, 49, 22, 20],\n",
       " [0, 48, 22, 20, 23],\n",
       " [0, 48, 22, 20, 17],\n",
       " [0, 48, 22, 20, 30],\n",
       " [0, 48, 22, 20, 5],\n",
       " [62, 48, 22, 20, 36],\n",
       " [61, 48, 22, 20],\n",
       " [66, 48, 36, 4, 16, 23],\n",
       " [0, 48, 36, 4, 16, 17],\n",
       " [0, 48, 36, 4, 16, 30],\n",
       " [0, 48, 36, 4, 16, 5],\n",
       " [65, 48, 36, 4, 16],\n",
       " [0, 48, 36, 4, 16, 23, 17],\n",
       " [0, 48, 36, 4, 16, 23, 30],\n",
       " [0, 48, 36, 4, 16, 23, 5],\n",
       " [66, 48, 36, 4, 16, 23],\n",
       " [0, 48, 36, 20, 16, 23],\n",
       " [0, 48, 36, 20, 16, 17],\n",
       " [0, 48, 36, 20, 16, 30],\n",
       " [0, 48, 36, 20, 16, 5],\n",
       " [0, 48, 36, 20, 16],\n",
       " [0, 49, 47, 39, 16, 23],\n",
       " [69, 49, 47, 39, 16, 17],\n",
       " [0, 49, 47, 39, 16, 30],\n",
       " [70, 49, 47, 39, 16, 5],\n",
       " [71, 49, 47, 39, 16, 36],\n",
       " [68, 49, 47, 39, 16],\n",
       " [0, 48, 1, 28, 4, 14, 23],\n",
       " [0, 48, 1, 28, 4, 14, 17],\n",
       " [0, 48, 1, 28, 4, 14, 30],\n",
       " [0, 48, 1, 28, 4, 14, 5],\n",
       " [73, 48, 1, 28, 4, 14, 36],\n",
       " [72, 48, 1, 28, 4, 14],\n",
       " [0, 48, 41, 28, 4, 14, 23],\n",
       " [0, 48, 41, 28, 4, 14, 17],\n",
       " [0, 48, 41, 28, 4, 14, 30],\n",
       " [0, 48, 41, 28, 4, 14, 5],\n",
       " [0, 48, 41, 28, 4, 14, 36],\n",
       " [74, 48, 41, 28, 4, 14],\n",
       " [0, 1, 28, 19, 14, 23],\n",
       " [0, 1, 28, 19, 14, 17],\n",
       " [0, 1, 28, 19, 14, 30],\n",
       " [0, 1, 28, 19, 14, 5],\n",
       " [0, 1, 28, 19, 14, 36],\n",
       " [0, 1, 28, 19, 14],\n",
       " [0, 7, 30, 24, 23],\n",
       " [0, 7, 30, 24, 17],\n",
       " [0, 7, 30, 24, 5],\n",
       " [76, 7, 30, 24, 36],\n",
       " [75, 7, 30, 24],\n",
       " [0, 48, 1, 30, 14, 23],\n",
       " [0, 48, 1, 30, 14, 17],\n",
       " [0, 48, 1, 30, 14, 5],\n",
       " [78, 48, 1, 30, 14, 36],\n",
       " [77, 48, 1, 30, 14],\n",
       " [0, 41, 30, 14, 23],\n",
       " [0, 41, 30, 14, 17],\n",
       " [0, 41, 30, 14, 5],\n",
       " [0, 41, 30, 14, 36],\n",
       " [79, 41, 30, 14],\n",
       " [0, 49, 7, 39, 24, 23],\n",
       " [0, 49, 7, 39, 24, 17],\n",
       " [0, 49, 7, 39, 24, 30],\n",
       " [0, 49, 7, 39, 24, 5],\n",
       " [84, 49, 7, 39, 24, 36],\n",
       " [82, 49, 7, 39, 24],\n",
       " [0, 49, 7, 20, 24, 23],\n",
       " [0, 49, 7, 20, 24, 17],\n",
       " [0, 49, 7, 20, 24, 30],\n",
       " [0, 49, 7, 20, 24, 5],\n",
       " [0, 49, 7, 20, 24, 36],\n",
       " [83, 49, 7, 20, 24],\n",
       " [0, 49, 46, 39, 23],\n",
       " [0, 49, 46, 39, 17],\n",
       " [0, 49, 46, 39, 30],\n",
       " [0, 49, 46, 39, 5],\n",
       " [0, 49, 46, 39, 36],\n",
       " [0, 49, 46, 39],\n",
       " [0, 1, 44, 14, 23],\n",
       " [0, 1, 44, 14, 17],\n",
       " [0, 1, 44, 14, 30],\n",
       " [0, 1, 44, 14, 5],\n",
       " [87, 1, 44, 14, 36],\n",
       " [85, 1, 44, 14],\n",
       " [0, 41, 19, 14, 23],\n",
       " [0, 41, 19, 14, 17],\n",
       " [0, 41, 19, 14, 30],\n",
       " [0, 41, 19, 14, 5],\n",
       " [0, 41, 19, 14, 36],\n",
       " [88, 41, 19, 14],\n",
       " [0, 1, 4, 14, 23],\n",
       " [0, 1, 4, 14, 17],\n",
       " [0, 1, 4, 14, 30],\n",
       " [0, 1, 4, 14, 5],\n",
       " [0, 1, 4, 14, 36],\n",
       " [0, 1, 4, 14],\n",
       " [0, 41, 4, 14, 23],\n",
       " [0, 41, 4, 14, 17],\n",
       " [0, 41, 4, 14, 30],\n",
       " [0, 41, 4, 14, 5],\n",
       " [0, 41, 4, 14, 36],\n",
       " [0, 41, 4, 14],\n",
       " [0, 49, 1, 44, 23],\n",
       " [0, 49, 1, 44, 17],\n",
       " [0, 49, 1, 44, 30],\n",
       " [0, 49, 1, 44, 5],\n",
       " [0, 49, 1, 44, 36],\n",
       " [0, 49, 1, 44],\n",
       " [0, 48, 1, 20, 44, 23],\n",
       " [0, 48, 1, 20, 44, 17],\n",
       " [0, 48, 1, 20, 44, 30],\n",
       " [0, 48, 1, 20, 44, 5],\n",
       " [0, 48, 1, 20, 44, 36],\n",
       " [86, 48, 1, 20, 44],\n",
       " [0, 49, 1, 20, 14, 23],\n",
       " [0, 49, 1, 20, 14, 17],\n",
       " [0, 49, 1, 20, 14, 30],\n",
       " [0, 49, 1, 20, 14, 5],\n",
       " [90, 49, 1, 20, 14, 36],\n",
       " [89, 49, 1, 20, 14],\n",
       " [0, 49, 41, 20, 14, 23],\n",
       " [0, 49, 41, 20, 14, 17],\n",
       " [0, 49, 41, 20, 14, 30],\n",
       " [0, 49, 41, 20, 14, 5],\n",
       " [0, 49, 41, 20, 14, 36],\n",
       " [91, 49, 41, 20, 14],\n",
       " [0, 49, 2, 20, 14, 23],\n",
       " [0, 49, 2, 20, 14, 17],\n",
       " [0, 49, 2, 20, 14, 30],\n",
       " [0, 49, 2, 20, 14, 5],\n",
       " [0, 49, 2, 20, 14, 36],\n",
       " [0, 49, 2, 20, 14],\n",
       " [0, 49, 1, 39, 14, 23],\n",
       " [0, 49, 1, 39, 14, 17],\n",
       " [0, 49, 1, 39, 14, 30],\n",
       " [0, 49, 1, 39, 14, 5],\n",
       " [95, 49, 1, 39, 14, 36],\n",
       " [92, 49, 1, 39, 14],\n",
       " [0, 49, 41, 39, 14, 23],\n",
       " [0, 49, 41, 39, 14, 17],\n",
       " [0, 49, 41, 39, 14, 30],\n",
       " [0, 49, 41, 39, 14, 5],\n",
       " [0, 49, 41, 39, 14, 36],\n",
       " [97, 49, 41, 39, 14],\n",
       " [0, 49, 41, 39, 20, 14, 23],\n",
       " [0, 49, 41, 39, 20, 14, 17],\n",
       " [0, 49, 41, 39, 20, 14, 30],\n",
       " [0, 49, 41, 39, 20, 14, 5],\n",
       " [0, 49, 41, 39, 20, 14, 36],\n",
       " [0, 49, 41, 39, 20, 14],\n",
       " [0, 48, 27, 20, 24, 23],\n",
       " [0, 48, 27, 20, 24, 17],\n",
       " [0, 48, 27, 20, 24, 30],\n",
       " [0, 48, 27, 20, 24, 5],\n",
       " [105, 48, 27, 20, 24, 36],\n",
       " [102, 48, 27, 20, 24],\n",
       " [0, 48, 27, 4, 23],\n",
       " [0, 48, 27, 4, 17],\n",
       " [0, 48, 27, 4, 30],\n",
       " [0, 48, 27, 4, 5],\n",
       " [104, 48, 27, 4, 36],\n",
       " [103, 48, 27, 4],\n",
       " [0, 26, 4, 24, 23],\n",
       " [0, 26, 4, 24, 17],\n",
       " [0, 26, 4, 24, 30],\n",
       " [0, 26, 4, 24, 5],\n",
       " [0, 26, 4, 24, 36],\n",
       " [107, 26, 4, 24],\n",
       " [0, 49, 47, 20, 16, 23],\n",
       " [0, 49, 47, 20, 16, 17],\n",
       " [0, 49, 47, 20, 16, 30],\n",
       " [0, 49, 47, 20, 16, 5],\n",
       " [109, 49, 47, 20, 16, 36],\n",
       " [108, 49, 47, 20, 16],\n",
       " [0, 49, 37, 20, 16, 23],\n",
       " [0, 49, 37, 20, 16, 17],\n",
       " [0, 49, 37, 20, 16, 30],\n",
       " [0, 49, 37, 20, 16, 5],\n",
       " [0, 49, 37, 20, 16, 36],\n",
       " [0, 49, 37, 20, 16],\n",
       " [0, 48, 1, 20, 14, 23],\n",
       " [0, 48, 1, 20, 14, 17],\n",
       " [0, 48, 1, 20, 14, 30],\n",
       " [0, 48, 1, 20, 14, 5],\n",
       " [115, 48, 1, 20, 14, 36],\n",
       " [114, 48, 1, 20, 14],\n",
       " [0, 48, 41, 20, 14, 23],\n",
       " [0, 48, 41, 20, 14, 17],\n",
       " [0, 48, 41, 20, 14, 30],\n",
       " [0, 48, 41, 20, 14, 5],\n",
       " [0, 48, 41, 20, 14, 36],\n",
       " [0, 48, 41, 20, 14],\n",
       " [0, 48, 2, 20, 23],\n",
       " [0, 48, 2, 20, 17],\n",
       " [0, 48, 2, 20, 30],\n",
       " [0, 48, 2, 20, 5],\n",
       " [0, 48, 2, 20, 36],\n",
       " [0, 48, 2, 20],\n",
       " [0, 48, 7, 20, 24, 23],\n",
       " [0, 48, 7, 20, 24, 17],\n",
       " [0, 48, 7, 20, 24, 30],\n",
       " [0, 48, 7, 20, 24, 5],\n",
       " [0, 48, 7, 20, 24, 36],\n",
       " [0, 48, 7, 20, 24],\n",
       " [0, 7, 44, 24, 23],\n",
       " [0, 7, 44, 24, 17],\n",
       " [0, 7, 44, 24, 30],\n",
       " [0, 7, 44, 24, 5],\n",
       " [0, 7, 44, 24, 36],\n",
       " [0, 7, 44, 24],\n",
       " [0, 49, 36, 20, 23],\n",
       " [0, 49, 36, 20, 17],\n",
       " [0, 49, 36, 20, 30],\n",
       " [0, 49, 36, 20, 5],\n",
       " [13, 49, 36, 20],\n",
       " [0, 48, 15, 20, 14, 23],\n",
       " [0, 48, 15, 20, 14, 17],\n",
       " [0, 48, 15, 20, 14, 30],\n",
       " [0, 48, 15, 20, 14, 5],\n",
       " [0, 48, 15, 20, 14, 36],\n",
       " [14, 48, 15, 20, 14],\n",
       " [0, 27, 30, 24, 23],\n",
       " [0, 27, 30, 24, 17],\n",
       " [0, 27, 30, 24, 5],\n",
       " [0, 27, 30, 24, 36],\n",
       " [0, 27, 30, 24],\n",
       " [0, 48, 47, 20, 16, 23],\n",
       " [0, 48, 47, 20, 16, 17],\n",
       " [0, 48, 47, 20, 16, 30],\n",
       " [0, 48, 47, 20, 16, 5],\n",
       " [0, 48, 47, 20, 16, 36],\n",
       " [18, 48, 47, 20, 16],\n",
       " [0, 48, 46, 39, 16, 23],\n",
       " [0, 48, 46, 39, 16, 17],\n",
       " [0, 48, 46, 39, 16, 30],\n",
       " [0, 48, 46, 39, 16, 5],\n",
       " [0, 48, 46, 39, 16, 36],\n",
       " [0, 48, 46, 39, 16],\n",
       " [0, 25, 4, 24, 23],\n",
       " [0, 25, 4, 24, 17],\n",
       " [0, 25, 4, 24, 30],\n",
       " [0, 25, 4, 24, 5],\n",
       " [0, 25, 4, 24, 36],\n",
       " [0, 25, 4, 24],\n",
       " [0, 49, 18, 20, 23],\n",
       " [0, 49, 18, 20, 17],\n",
       " [0, 49, 18, 20, 30],\n",
       " [0, 49, 18, 20, 5],\n",
       " [0, 49, 18, 20, 36],\n",
       " [0, 49, 18, 20],\n",
       " [0, 36, 30, 23],\n",
       " [0, 36, 30, 17],\n",
       " [0, 36, 30, 5],\n",
       " [20, 36, 30],\n",
       " [0, 48, 36, 39, 23],\n",
       " [0, 48, 36, 39, 17],\n",
       " [0, 48, 36, 39, 30],\n",
       " [0, 48, 36, 39, 5],\n",
       " [22, 48, 36, 39, 29],\n",
       " [21, 48, 36, 39],\n",
       " [0, 49, 1, 28, 20, 14, 23],\n",
       " [0, 49, 1, 28, 20, 14, 17],\n",
       " [0, 49, 1, 28, 20, 14, 30],\n",
       " [0, 49, 1, 28, 20, 14, 5],\n",
       " [0, 49, 1, 28, 20, 14, 36],\n",
       " [0, 49, 1, 28, 20, 14],\n",
       " [0, 48, 1, 28, 20, 14, 23],\n",
       " [0, 48, 1, 28, 20, 14, 17],\n",
       " [0, 48, 1, 28, 20, 14, 30],\n",
       " [0, 48, 1, 28, 20, 14, 5],\n",
       " [0, 48, 1, 28, 20, 14, 36],\n",
       " [0, 48, 1, 28, 20, 14],\n",
       " [0, 36, 28, 4, 23],\n",
       " [0, 36, 28, 4, 17],\n",
       " [0, 36, 28, 4, 30],\n",
       " [0, 36, 28, 4, 5],\n",
       " [23, 36, 28, 4],\n",
       " [0, 47, 28, 4, 16, 23],\n",
       " [0, 47, 28, 4, 16, 17],\n",
       " [0, 47, 28, 4, 16, 30],\n",
       " [0, 47, 28, 4, 16, 5],\n",
       " [0, 47, 28, 4, 16, 36],\n",
       " [0, 47, 28, 4, 16],\n",
       " [0, 47, 4, 16, 23],\n",
       " [0, 47, 4, 16, 17],\n",
       " [0, 47, 4, 16, 30],\n",
       " [0, 47, 4, 16, 5],\n",
       " [0, 47, 4, 16, 36],\n",
       " [0, 47, 4, 16],\n",
       " [0, 47, 30, 16, 23],\n",
       " [0, 47, 30, 16, 17],\n",
       " [0, 47, 30, 16, 5],\n",
       " [0, 47, 30, 16, 36],\n",
       " [25, 47, 30, 16],\n",
       " [0, 46, 30, 16, 23],\n",
       " [0, 46, 30, 16, 17],\n",
       " [0, 46, 30, 16, 5],\n",
       " [0, 46, 30, 16, 36],\n",
       " [0, 46, 30, 16],\n",
       " [0, 7, 10, 24, 23],\n",
       " [0, 7, 10, 24, 17],\n",
       " [0, 7, 10, 24, 30],\n",
       " [0, 7, 10, 24, 5],\n",
       " [0, 7, 10, 24, 36],\n",
       " [0, 7, 10, 24],\n",
       " [0, 27, 4, 24, 23],\n",
       " [0, 27, 4, 24, 17],\n",
       " [0, 27, 4, 24, 30],\n",
       " [0, 27, 4, 24, 5],\n",
       " [0, 27, 4, 24, 36],\n",
       " [0, 27, 4, 24],\n",
       " [0, 48, 46, 20, 23],\n",
       " [0, 48, 46, 20, 17],\n",
       " [0, 48, 46, 20, 30],\n",
       " [0, 48, 46, 20, 5],\n",
       " [0, 48, 46, 20, 36],\n",
       " [0, 48, 46, 20],\n",
       " [0, 46, 44, 23],\n",
       " [0, 46, 44, 17],\n",
       " [0, 46, 44, 30],\n",
       " [0, 46, 44, 5],\n",
       " [0, 46, 44, 36],\n",
       " [0, 46, 44],\n",
       " [0, 49, 40, 20, 14, 23],\n",
       " [0, 49, 40, 20, 14, 17],\n",
       " [0, 49, 40, 20, 14, 30],\n",
       " [0, 49, 40, 20, 14, 5],\n",
       " [29, 49, 40, 20, 14, 36],\n",
       " [28, 49, 40, 20, 14],\n",
       " [0, 49, 40, 20, 14, 3, 23],\n",
       " [0, 49, 40, 20, 14, 3, 17],\n",
       " [0, 49, 40, 20, 14, 3, 30],\n",
       " [0, 49, 40, 20, 14, 3, 5],\n",
       " [0, 49, 40, 20, 14, 3, 36],\n",
       " [0, 49, 40, 20, 14, 3],\n",
       " [0, 49, 15, 20, 14, 23],\n",
       " [0, 49, 15, 20, 14, 17],\n",
       " [0, 49, 15, 20, 14, 30],\n",
       " [0, 49, 15, 20, 14, 5],\n",
       " [0, 49, 15, 20, 14, 36],\n",
       " [30, 49, 15, 20, 14],\n",
       " [0, 49, 26, 20, 24, 23],\n",
       " [0, 49, 26, 20, 24, 17],\n",
       " [0, 49, 26, 20, 24, 30],\n",
       " [0, 49, 26, 20, 24, 5],\n",
       " [0, 49, 26, 20, 24, 36],\n",
       " [0, 49, 26, 20, 24],\n",
       " [0, 49, 46, 20, 23],\n",
       " [0, 49, 46, 20, 17],\n",
       " [0, 49, 46, 20, 30],\n",
       " [0, 49, 46, 20, 5],\n",
       " [0, 49, 46, 20, 36],\n",
       " [0, 49, 46, 20],\n",
       " [0, 49, 38, 20, 23],\n",
       " [0, 49, 38, 20, 17],\n",
       " [0, 49, 38, 20, 30],\n",
       " [0, 49, 38, 20, 5],\n",
       " [0, 49, 38, 20, 36],\n",
       " [0, 49, 38, 20],\n",
       " [0, 48, 40, 20, 14, 23],\n",
       " [0, 48, 40, 20, 14, 17],\n",
       " [0, 48, 40, 20, 14, 30],\n",
       " [0, 48, 40, 20, 14, 5],\n",
       " [33, 48, 40, 20, 14, 36],\n",
       " [32, 48, 40, 20, 14],\n",
       " [0, 1, 19, 14, 23],\n",
       " [0, 1, 19, 14, 17],\n",
       " [0, 1, 19, 14, 30],\n",
       " [0, 1, 19, 14, 5],\n",
       " [0, 1, 19, 14, 36],\n",
       " [5, 1, 19, 14],\n",
       " [0, 47, 1, 28, 4, 14, 16, 23],\n",
       " [0, 47, 1, 28, 4, 14, 16, 17],\n",
       " [0, 47, 1, 28, 4, 14, 16, 30],\n",
       " [0, 47, 1, 28, 4, 14, 16, 5],\n",
       " [0, 47, 1, 28, 4, 14, 16, 36],\n",
       " [0, 47, 1, 28, 4, 14, 16],\n",
       " [0, 22, 43, 23],\n",
       " [0, 22, 43, 17],\n",
       " [0, 22, 43, 30],\n",
       " [0, 22, 43, 5],\n",
       " [9, 22, 43, 36],\n",
       " [0, 22, 43],\n",
       " [0, 48, 38, 20, 23],\n",
       " [0, 48, 38, 20, 17],\n",
       " [0, 48, 38, 20, 30],\n",
       " [0, 48, 38, 20, 5],\n",
       " [0, 48, 38, 20, 36],\n",
       " [0, 48, 38, 20],\n",
       " [0, 15, 10, 14, 23],\n",
       " [0, 15, 10, 14, 17],\n",
       " [0, 15, 10, 14, 30],\n",
       " [0, 15, 10, 14, 5],\n",
       " [0, 15, 10, 14, 36],\n",
       " [0, 15, 10, 14],\n",
       " [0, 1, 28, 10, 14, 23],\n",
       " [0, 1, 28, 10, 14, 17],\n",
       " [0, 1, 28, 10, 14, 30],\n",
       " [0, 1, 28, 10, 14, 5],\n",
       " [0, 1, 28, 10, 14, 36],\n",
       " [0, 1, 28, 10, 14],\n",
       " [0, 36, 10, 23],\n",
       " [0, 36, 10, 17],\n",
       " [0, 36, 10, 30],\n",
       " [0, 36, 10, 5],\n",
       " [0, 36, 10],\n",
       " [0, 49, 1, 0, 14, 23],\n",
       " [0, 49, 1, 0, 14, 17],\n",
       " [0, 49, 1, 0, 14, 30],\n",
       " [0, 49, 1, 0, 14, 5],\n",
       " [99, 49, 1, 0, 14, 36],\n",
       " [98, 49, 1, 0, 14],\n",
       " [0, 48, 1, 0, 14, 23],\n",
       " [0, 48, 1, 0, 14, 17],\n",
       " [0, 48, 1, 0, 14, 30],\n",
       " [0, 48, 1, 0, 14, 5],\n",
       " [50, 48, 1, 0, 14, 36],\n",
       " [49, 48, 1, 0, 14],\n",
       " [0, 48, 2, 0, 14, 23],\n",
       " [0, 48, 2, 0, 14, 17],\n",
       " [0, 48, 2, 0, 14, 30],\n",
       " [0, 48, 2, 0, 14, 5],\n",
       " [0, 48, 2, 0, 14, 36],\n",
       " [0, 48, 2, 0, 14],\n",
       " [0, 48, 40, 0, 14, 23],\n",
       " [0, 48, 40, 0, 14, 17],\n",
       " [0, 48, 40, 0, 14, 30],\n",
       " [0, 48, 40, 0, 14, 5],\n",
       " [0, 48, 40, 0, 14, 36],\n",
       " [44, 48, 40, 0, 14],\n",
       " [0, 49, 40, 0, 14, 23],\n",
       " [0, 49, 40, 0, 14, 17],\n",
       " [0, 49, 40, 0, 14, 30],\n",
       " [0, 49, 40, 0, 14, 5],\n",
       " [94, 49, 40, 0, 14, 36],\n",
       " [93, 49, 40, 0, 14],\n",
       " [0, 49, 2, 0, 14, 23],\n",
       " [0, 49, 2, 0, 14, 17],\n",
       " [0, 49, 2, 0, 14, 30],\n",
       " [0, 49, 2, 0, 14, 5],\n",
       " [0, 49, 2, 0, 14, 36],\n",
       " [96, 49, 2, 0, 14],\n",
       " [0, 49, 36, 20, 16, 23],\n",
       " [0, 49, 36, 20, 16, 17],\n",
       " [0, 49, 36, 20, 16, 30],\n",
       " [0, 49, 36, 20, 16, 5],\n",
       " [0, 49, 36, 20, 16],\n",
       " [0, 49, 1, 28, 0, 14, 23],\n",
       " [0, 49, 1, 28, 0, 14, 17],\n",
       " [0, 49, 1, 28, 0, 14, 30],\n",
       " [0, 49, 1, 28, 0, 14, 5],\n",
       " [0, 49, 1, 28, 0, 14, 36],\n",
       " [0, 49, 1, 28, 0, 14],\n",
       " [0, 48, 2, 0, 23],\n",
       " [0, 48, 2, 0, 17],\n",
       " [0, 48, 2, 0, 30],\n",
       " [0, 48, 2, 0, 5],\n",
       " [0, 48, 2, 0, 36],\n",
       " [47, 48, 2, 0]]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_variables(filename=None, load_dir=None):\n",
    "    if load_dir is None:\n",
    "        load_dir = os.getcwd()\n",
    "    if filename is None:\n",
    "        load_path = join(load_dir, \"feat_vars.pkl\")\n",
    "    else:\n",
    "        load_path = join(load_dir, filename + \".pkl\")\n",
    "    with open(load_path, 'rb') as f:\n",
    "        feat_idx_dict, vp, cp, vp_idx, cp_idx = pickle.load(f)\n",
    "    return feat_idx_dict, vp, cp, vp_idx, cp_idx\n",
    "\n",
    "feat_idx_dict, vp, cp, vp_idx, cp_idx = load_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([58, 248])\n",
      "torch.Size([505, 385])\n"
     ]
    }
   ],
   "source": [
    "def convert_to_pred(out, feat_idx_dict, vp, cp, vp_idx, cp_idx):\n",
    "    # split into silence, vowels and consonants\n",
    "    vowel_idx = feat_idx_dict[\"vowel\"]\n",
    "    consonant_idx = feat_idx_dict[\"consonant\"]\n",
    "    vowels = out[:, vowel_idx]\n",
    "    consonants = out[:, consonant_idx]\n",
    "    sums = vowels + consonants\n",
    "    # There is roughly a 50% chance this phone is non-silence.\n",
    "    sum_bools = sums > 0.5\n",
    "    # Take max of vowel vs. consonant\n",
    "    vowel_bools = vowels > consonants\n",
    "    consonant_bools = ~vowel_bools\n",
    "    \n",
    "    vowel_idx = sum_bools & vowel_bools \n",
    "    consonant_idx = sum_bools & consonant_bools\n",
    "    silence_idx = ~sum_bools\n",
    "\n",
    "    vowels = out[vowel_idx]\n",
    "    consonants = out[consonant_idx]\n",
    "\n",
    "    # take the slice from [1:] (all the atts idx)\n",
    "    # sum the value of these attributes. \n",
    "    # divide by the number of attributes to normalise\n",
    "    v_scores = [vowels[:,slice_idx[1:]].sum(dim=1)/(len(slice_idx)-1) for slice_idx in vp]\n",
    "    # Stack to shape [29, N], where each 29 is the score for each vowel\n",
    "    v_scores = torch.stack(v_scores, dim=0)\n",
    "    print(v_scores.size())\n",
    "    # Get max phone idx, and convert this to the corresponding phone\n",
    "    v_out = vp_idx[v_scores.argmax(dim=0)] \n",
    "\n",
    "    c_scores = [consonants[:,slice_idx[1:]].sum(dim=1)/(len(slice_idx)-1) for slice_idx in cp]\n",
    "    # Stack to shape [86, N] where each 86 is the score for each consonant\n",
    "    c_scores = torch.stack(c_scores, dim=0)\n",
    "    print(c_scores.size())\n",
    "    c_out = cp_idx[c_scores.argmax(dim=0)]\n",
    "    \n",
    "    # Fill final output\n",
    "    final_out = torch.zeros(out.size()[0]).cuda()\n",
    "    final_out.masked_scatter_(consonant_idx, c_out.cuda().float())\n",
    "    final_out.masked_scatter_(vowel_idx, v_out.cuda().float())\n",
    "    final_out.masked_fill_(silence_idx, 1)\n",
    "    final_out = final_out.int()\n",
    "    \n",
    "    return final_out\n",
    "\n",
    "pred = convert_to_pred(out, feat_idx_dict, vp, cp, vp_idx, cp_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_scores(out, feat_idx_dict, phone_idx_dict, vp, cp, vp_idx, cp_idx):\n",
    "    # split into silence, vowels and consonants\n",
    "    vowel_idx = feat_idx_dict[\"vowel\"]\n",
    "    consonant_idx = feat_idx_dict[\"consonant\"]\n",
    "    vowels = out[:, vowel_idx]\n",
    "    consonants = out[:, consonant_idx]\n",
    "    sums = vowels + consonants\n",
    "    # There is roughly a 50% chance this phone is non-silence.\n",
    "    sum_bools = sums > 0.5\n",
    "    # Take max of vowel vs. consonant\n",
    "    vowel_bools = vowels > consonants\n",
    "    consonant_bools = ~vowel_bools\n",
    "    \n",
    "    vowel_idx = sum_bools & vowel_bools \n",
    "    consonant_idx = sum_bools & consonant_bools\n",
    "    silence_idx = ~sum_bools\n",
    "\n",
    "    vowels = out[vowel_idx]\n",
    "    consonants = out[consonant_idx]\n",
    "    \n",
    "    n_phones = len(phone_idx_dict)\n",
    "    \n",
    "    # Output is N x #phones\n",
    "    out_scores = torch.zeros((out.size()[0], n_phones))\n",
    "    \n",
    "    combined_v_scores = get_all_phone_scores(vowels, vp, vp_idx, n_phones)\n",
    "    combined_c_scores = get_all_phone_scores(consonants, cp, cp_idx, n_phones)\n",
    "\n",
    "    # Fill final output\n",
    "    final_out = torch.zeros((out.size()[0], n_phones)).cuda()\n",
    "    final_out[consonant_idx,:] =  combined_c_scores\n",
    "    final_out[vowel_idx,:] = combined_v_scores\n",
    "    final_out[silence_idx,1] = 1 - sums[~sum_bools]  # Fill the silent indices with the probability they were silence\n",
    "    final_out = final_out.cuda()\n",
    "    \n",
    "    return final_out, sums\n",
    "\n",
    "def get_all_phone_scores(out, phones, phone_idx, total_seen_phones):\n",
    "    # take the slice from [1:] (all the atts idx)\n",
    "    # sum the value of these attributes. \n",
    "    # divide by the number of attributes to normalise\n",
    "    scores = [out[:,slice_idx[1:]].sum(dim=1)/(len(slice_idx)-1) for slice_idx in phones]\n",
    "    # Stack to shape [# vowel phones, N]\n",
    "    scores = torch.stack(scores, dim=0)\n",
    "    n = scores.shape[1]\n",
    "    \n",
    "    scores, phone_idx = reduce_zeros(scores, phone_idx)\n",
    "    \n",
    "    combined_scores = torch.zeros((n, total_seen_phones))\n",
    "    for idx, col_idx in enumerate(phone_idx.long()):\n",
    "        combined_scores[:,col_idx] = scores[idx,:]\n",
    "        \n",
    "    return combined_scores.cuda()\n",
    "        \n",
    "    \n",
    "\n",
    "final_out, sums = convert_to_scores(out, feat_idx_dict, phone_idx_dict, vp, cp, vp_idx, cp_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_sums(tensor_list, target_sum=None):\n",
    "    init_shape = tensor_list[0].size()\n",
    "    if target_sum is None:\n",
    "        target_sum = init_shape[0]\n",
    "    sum_true_vals = 0\n",
    "    for tensor in tensor_list:\n",
    "        assert tensor.size() == init_shape, \"Tensors mismatch in size\"\n",
    "        sum_true_vals += tensor.unique(return_counts=True)[1][1].item()\n",
    "    return sum_true_vals == target_sum\n",
    "    \n",
    "check_sums([vowel_idx, consonant_idx, silence_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "def split_on_max(atts_list, tensor, feat_idx_dict):\n",
    "    # Get indices corresponding to articulatory features in question\n",
    "    slice_idx = [feat_idx_dict[x] for x in atts_list]\n",
    "    print(slice_idx)\n",
    "    \n",
    "\n",
    "atts_list = [\"front\", \"back\", \"central\"]\n",
    "split_on_max(atts_list, vowels, feat_idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'front'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height_list.sort()\n",
    "height_idx = [feat_idx_dict[x] for x in height_list]\n",
    "max_idx = test[height_idx].argmax()\n",
    "height_list[max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "schwa_idx = [feat_idx_dict[x] for x in [\"close-mid\", \"open-mid\", \"rounded\", \"unrounded\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5613911182736047"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test[schwa_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 'a', 'vowel', 'open', 'front', 'unrounded'],\n",
       " [5, 'e', 'vowel', 'close-mid', 'front', 'unrounded'],\n",
       " [11, 'i', 'vowel', 'close', 'front', 'unrounded'],\n",
       " [43, 'y', 'vowel', 'close', 'front', 'rounded'],\n",
       " [52, 'E', 'vowel', 'open-mid', 'front', 'unrounded'],\n",
       " [58, 'I', 'vowel', 'near-close', 'front', 'unrounded'],\n",
       " [85, 'Y', 'vowel', 'near-close', 'front', 'rounded'],\n",
       " [88, '{', 'vowel', 'near-open', 'front', 'unrounded'],\n",
       " [91, '2', 'vowel', 'close-mid', 'front', 'rounded'],\n",
       " [99, '9', 'vowel', 'open-mid', 'front', 'rounded'],\n",
       " [100, '&', 'vowel', 'open', 'front', 'rounded']]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dict[\"front\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "front\n"
     ]
    }
   ],
   "source": [
    "def classify_vowel(phone, height_list, height_idx, openness_list, openness_idx, root_dict):\n",
    "    # Get most likely height (front/back/central)\n",
    "    height = height_list[test[height_idx].argmax()]\n",
    "    if height != \"central\":\n",
    "        \n",
    "    \n",
    "\n",
    "classify_vowel(test, height_list, height_idx, openness_list, openness_idx, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([False], device='cuda:0'), tensor([800], device='cuda:0'))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out[:, schwa_idx].sum(dim=1) > 1).unique(return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'front': {'close',\n",
       "  'close-mid',\n",
       "  'near-close',\n",
       "  'near-open',\n",
       "  'open',\n",
       "  'open-mid'},\n",
       " 'back': {'close', 'close-mid', 'near-close', 'open', 'open-mid'},\n",
       " 'central': {'close', 'close-mid', 'near-close', 'near-open', 'open-mid'}}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_step_dict = {}\n",
    "for height, phones in height_dict.items():\n",
    "    subsplit = check_phones_split(openness_list, phones)\n",
    "    subkeys = set()\n",
    "    for key in subsplit:\n",
    "        subkeys.add(key)\n",
    "    next_step_dict[height] = subkeys\n",
    "next_step_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{21: [array([34, 34, 21, 45]), array([52, 12, 21, 45]), array([63, 11, 21, 45]), array([110,  11,  21,  42]), array([15, 35, 21, 45]), array([19, 31, 21, 45]), array([ 0, 31, 21, 42]), array([117,  32,  21,  45]), array([ 4, 12, 21, 42]), array([ 7, 35, 21, 42]), array([ 0, 34, 21, 42])], 6: [array([80, 12,  6, 42]), array([100,  11,   6,  42]), array([11, 34,  6, 45]), array([24, 11,  6, 45]), array([26, 35,  6, 42]), array([ 0, 34,  6, 42]), array([31, 31,  6, 42]), array([ 0, 35,  6, 45]), array([ 6, 12,  6, 45])], 9: [array([ 0, 31,  9, 45]), array([ 0, 31,  9, 42]), array([10, 12, 35,  9]), array([ 0, 11,  9, 42]), array([ 3, 11,  9, 45]), array([ 0, 35,  9, 45]), array([ 0, 35,  9, 42]), array([ 0, 32,  9]), array([ 0, 12,  9, 42])]}\n"
     ]
    }
   ],
   "source": [
    "def setup(feat_idx_dict):\n",
    "    openness_list = [\"open\", \"close-mid\", \"close\", \"open-mid\", \"near-close\", \"near-open\"]\n",
    "    height_list = [\"front\", \"back\", \"central\"]\n",
    "    height_idx = [feat_idx_dict[x] for x in height_list]\n",
    "    openness_idx = [feat_idx_dict[x] for x in openness_list]\n",
    "    return height_list, height_idx, openness_list, openness_idx\n",
    "\n",
    "def check_phones_split(atts_list, phones, att_vec_idx=None):\n",
    "    out_split = {}\n",
    "    for att_idx, att in enumerate(atts_list):\n",
    "        matching_phones = []\n",
    "        for phone in phones:\n",
    "            # Need to do [1:] since first number is phone index itself\n",
    "            if att in phone[1:]:\n",
    "                matching_phones.append(phone)\n",
    "        if len(matching_phones) > 0:\n",
    "            # Can use the index instead of the name \n",
    "            if att_vec_idx is not None:\n",
    "                out_split[att_vec_idx[att_idx]] = matching_phones\n",
    "            else:\n",
    "                out_split[att] = matching_phones\n",
    "    return out_split\n",
    "\n",
    "\n",
    "def build_dict_tree(phones, feat_idx_dict, height_list, openness_list):\n",
    "    root_dict = {} \n",
    "    height_idx = [feat_idx_dict[x] for x in height_list]\n",
    "    openness_idx = [feat_idx_dict[x] for x in openness_list]\n",
    "    root_dict[\"operation\"] = \"max\"\n",
    "    root_dict[\"indices\"] = height_idx\n",
    "    \n",
    "    height_dict = check_phones_split(height_idx, phones, att_vec_idx=height_idx)\n",
    "    print(height_dict)\n",
    "    \n",
    "\n",
    "build_dict_tree(vp, feat_idx_dict, height_list, openness_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "front\n",
      "[[0, 'a', 'vowel', 'open', 'front', 'unrounded'], [100, '&', 'vowel', 'open', 'front', 'rounded']]\n",
      "[[5, 'e', 'vowel', 'close-mid', 'front', 'unrounded'], [91, '2', 'vowel', 'close-mid', 'front', 'rounded']]\n",
      "[[11, 'i', 'vowel', 'close', 'front', 'unrounded'], [43, 'y', 'vowel', 'close', 'front', 'rounded']]\n",
      "[[52, 'E', 'vowel', 'open-mid', 'front', 'unrounded'], [99, '9', 'vowel', 'open-mid', 'front', 'rounded']]\n",
      "[[58, 'I', 'vowel', 'near-close', 'front', 'unrounded'], [85, 'Y', 'vowel', 'near-close', 'front', 'rounded']]\n",
      "[[88, '{', 'vowel', 'near-open', 'front', 'unrounded']]\n",
      "back\n",
      "[[47, 'A', 'vowel', 'open', 'back', 'unrounded'], [73, 'Q', 'vowel', 'open', 'back', 'rounded']]\n",
      "[[21, 'o', 'vowel', 'close-mid', 'back', 'rounded'], [97, '7', 'vowel', 'close-mid', 'back', 'unrounded']]\n",
      "[[37, 'u', 'vowel', 'close', 'back', 'rounded'], [66, 'M', 'vowel', 'close', 'back', 'unrounded']]\n",
      "[[70, 'O', 'vowel', 'open-mid', 'back', 'rounded'], [81, 'V', 'vowel', 'open-mid', 'back', 'unrounded']]\n",
      "[[79, 'U', 'vowel', 'near-close', 'back', 'rounded']]\n",
      "central\n",
      "[[87, '@', 'vowel', 'close-mid', 'open-mid', 'central'], [98, '8', 'vowel', 'close-mid', 'central', 'rounded']]\n",
      "[[89, '}', 'vowel', 'close', 'central', 'rounded'], [90, '1', 'vowel', 'close', 'central', 'unrounded']]\n",
      "[[87, '@', 'vowel', 'close-mid', 'open-mid', 'central'], [92, '3', 'vowel', 'open-mid', 'central', 'unrounded'], [93, '3\\\\', 'vowel', 'open-mid', 'central', 'rounded']]\n",
      "[[59, 'I\\\\', 'vowel', 'near-close', 'central', 'unrounded'], [80, 'U\\\\', 'vowel', 'near-close', 'central', 'rounded']]\n",
      "[[96, '6', 'vowel', 'near-open', 'central']]\n"
     ]
    }
   ],
   "source": [
    "for height, phones in height_dict.items():\n",
    "    print(height)\n",
    "    subsplit = check_phones_split(openness_list, phones)\n",
    "    for sub_phones in subsplit.values():\n",
    "        print(sub_phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vowel_phone_tree = {\n",
    "    \"front\": {\n",
    "        \"close\": {\n",
    "            \"rounded\": {\"y\"},\n",
    "            \"unrounded\": {\"i\"}            \n",
    "        },\n",
    "        \"near-close\": {\n",
    "            \"rounded\": {\"Y\"},\n",
    "            \"unrounded\": {\"I\"}            \n",
    "        },\n",
    "        \"near-open\": {\"{\"}, \n",
    "        \"open\": {\n",
    "            \"rounded\": {\"&\"},\n",
    "            \"unrounded\": {\"a\"}\n",
    "        },\n",
    "        \"open-mid\": {\n",
    "            \"rounded\": {\"9\"},\n",
    "            \"unrounded\": {\"E\"}\n",
    "        }\n",
    "    },\n",
    "    \"back\": {\n",
    "        \"close\": {},\n",
    "        \"close-mid\": {},\n",
    "        \"near-close:\": {},\n",
    "        \"open\": {},\n",
    "        \"open-mid\": {}\n",
    "    },\n",
    "    \"central\": {\n",
    "        \"close\": {},\n",
    "        \"close-mid\": {},\n",
    "        \"near-close\": {},\n",
    "        \"near-open\": {},\n",
    "        \"open-mid\": {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For splitting the last set of phones\n",
    "def final_split_rounded(phones):\n",
    "    n = len(phones)\n",
    "    # Only one phone, don't need to split\n",
    "    if n == 1:\n",
    "        return phones\n",
    "    elif n == 2:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "front\n",
      "[[0, 'a', 'vowel', 'open', 'front', 'unrounded'], [100, '&', 'vowel', 'open', 'front', 'rounded']]\n",
      "[[5, 'e', 'vowel', 'close-mid', 'front', 'unrounded'], [91, '2', 'vowel', 'close-mid', 'front', 'rounded']]\n",
      "[[11, 'i', 'vowel', 'close', 'front', 'unrounded'], [43, 'y', 'vowel', 'close', 'front', 'rounded']]\n",
      "[[52, 'E', 'vowel', 'open-mid', 'front', 'unrounded'], [99, '9', 'vowel', 'open-mid', 'front', 'rounded']]\n",
      "[[58, 'I', 'vowel', 'near-close', 'front', 'unrounded'], [85, 'Y', 'vowel', 'near-close', 'front', 'rounded']]\n",
      "[[88, '{', 'vowel', 'near-open', 'front', 'unrounded']]\n",
      "back\n",
      "[[47, 'A', 'vowel', 'open', 'back', 'unrounded'], [73, 'Q', 'vowel', 'open', 'back', 'rounded']]\n",
      "[[21, 'o', 'vowel', 'close-mid', 'back', 'rounded'], [97, '7', 'vowel', 'close-mid', 'back', 'unrounded']]\n",
      "[[37, 'u', 'vowel', 'close', 'back', 'rounded'], [66, 'M', 'vowel', 'close', 'back', 'unrounded']]\n",
      "[[70, 'O', 'vowel', 'open-mid', 'back', 'rounded'], [81, 'V', 'vowel', 'open-mid', 'back', 'unrounded']]\n",
      "[[79, 'U', 'vowel', 'near-close', 'back', 'rounded']]\n",
      "central\n",
      "[[87, '@', 'vowel', 'close-mid', 'open-mid', 'central'], [98, '8', 'vowel', 'close-mid', 'central', 'rounded']]\n",
      "[[89, '}', 'vowel', 'close', 'central', 'rounded'], [90, '1', 'vowel', 'close', 'central', 'unrounded']]\n",
      "[[87, '@', 'vowel', 'close-mid', 'open-mid', 'central'], [92, '3', 'vowel', 'open-mid', 'central', 'unrounded'], [93, '3\\\\', 'vowel', 'open-mid', 'central', 'rounded']]\n",
      "[[59, 'I\\\\', 'vowel', 'near-close', 'central', 'unrounded'], [80, 'U\\\\', 'vowel', 'near-close', 'central', 'rounded']]\n",
      "[[96, '6', 'vowel', 'near-open', 'central']]\n"
     ]
    }
   ],
   "source": [
    "for height, phones in height_dict.items():\n",
    "    print(height)\n",
    "    subsplit = check_phones_split(openness_list, phones)\n",
    "    for sub_phones in subsplit.values():\n",
    "        print(sub_phones)\n",
    "        #if len(sub_phones) > 2:\n",
    "        #    print(sub_phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_set(input_set):\n",
    "    line = \"\"\n",
    "    for attribute in input_set:\n",
    "        line += attribute + \" \"\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "velar voiced alveolar glottal lateral coronal dorsal labial-palatal trill stop palatal apical dental plosive approximant bilabial postalveolar pharyngeal fricative affricate voiceless labial alveolo-palatal flap epiglottal labial-velar nasal click palatal-velar uvular retroflex labiodental \n"
     ]
    }
   ],
   "source": [
    "line = \"\"\n",
    "for attribute in ca:\n",
    "    line += attribute + \" \"\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'place_general' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-f123ef80b7a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfull_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mplace_general\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0matt_phones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mphone\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0matt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mphone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'place_general' is not defined"
     ]
    }
   ],
   "source": [
    "full_dict = {}\n",
    "for att in place_general:\n",
    "    att_phones = []\n",
    "    for phone in cp:\n",
    "        if att in phone:\n",
    "            att_phones.append(phone)\n",
    "    full_dict[att] = att_phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voiced labial-velar affricate apical click voiceless plosive \n"
     ]
    }
   ],
   "source": [
    "place_top = {\"coronal\", \"labial\", \"dorsal\", \"pharyngeal\", \"glottal\", \"epiglottal\", \"alveolo-palatal\", \"labiodental\"}\n",
    "place_coronal = {\"dental\", \"alveolar\", \"postalveolar\", \"retroflex\", \"palatal\"}\n",
    "place_dorsal = {\"palatal\", \"velar\", \"uvular\", \"palatal-velar\"}\n",
    "place_labial = {\"bilabial\", \"labiodental\", \"labial-palatal\"}\n",
    "manner = {\"nasal\", \"stop\", \"fricative\", \"approximant\", \"flap\", \"trill\", \"lateral\"}\n",
    "\n",
    "remaining_atts = set()\n",
    "for att in ca:\n",
    "    selected_atts = place_top.union(manner)\n",
    "    selected_atts = selected_atts.union(place_coronal)\n",
    "    selected_atts = selected_atts.union(place_dorsal)\n",
    "    selected_atts = selected_atts.union(place_labial)\n",
    "    if att not in selected_atts:\n",
    "        remaining_atts.add(att)\n",
    "\n",
    "print_set(remaining_atts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BinaryTree(object):\n",
    "    def __init__(self, key):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.key = key\n",
    "        self.split_idx = None\n",
    "        self.left_is_leaf = False\n",
    "        self.right_is_leaf = False\n",
    "        \n",
    "    def display(self):\n",
    "        lines, _, _, _ = self._display_aux()\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "            \n",
    "    def save(self, filepath):\n",
    "        lines, _, _, _ = self._display_aux()\n",
    "        with open(filepath, \"w\") as f:\n",
    "            for line in lines:\n",
    "                f.write(line + \"\\n\")\n",
    "\n",
    "    def _display_aux(self):\n",
    "        \"\"\"Returns list of strings, width, height, and horizontal coordinate of the root.\"\"\"\n",
    "        # No child.\n",
    "        if self.right is None and self.left is None:\n",
    "            line = '%s' % self.key\n",
    "            width = len(line)\n",
    "            height = 1\n",
    "            middle = width // 2\n",
    "            return [line], width, height, middle\n",
    "\n",
    "        # Only left child.\n",
    "        if self.right is None:\n",
    "            lines, n, p, x = self.left._display_aux()\n",
    "            s = '%s' % self.key\n",
    "            u = len(s)\n",
    "            first_line = (x + 1) * ' ' + (n - x - 1) * '_' + s\n",
    "            second_line = x * ' ' + '/' + (n - x - 1 + u) * ' '\n",
    "            shifted_lines = [line + u * ' ' for line in lines]\n",
    "            return [first_line, second_line] + shifted_lines, n + u, p + 2, n + u // 2\n",
    "\n",
    "        # Only right child.\n",
    "        if self.left is None:\n",
    "            lines, n, p, x = self.right._display_aux()\n",
    "            s = '%s' % self.key\n",
    "            u = len(s)\n",
    "            first_line = s + x * '_' + (n - x) * ' '\n",
    "            second_line = (u + x) * ' ' + '\\\\' + (n - x - 1) * ' '\n",
    "            shifted_lines = [u * ' ' + line for line in lines]\n",
    "            return [first_line, second_line] + shifted_lines, n + u, p + 2, u // 2\n",
    "\n",
    "        # Two children.\n",
    "        left, n, p, x = self.left._display_aux()\n",
    "        right, m, q, y = self.right._display_aux()\n",
    "        s = '%s' % self.key\n",
    "        u = len(s)\n",
    "        first_line = (x + 1) * ' ' + (n - x - 1) * '_' + s + y * '_' + (m - y) * ' '\n",
    "        second_line = x * ' ' + '/' + (n - x - 1 + u + y) * ' ' + '\\\\' + (m - y - 1) * ' '\n",
    "        if p < q:\n",
    "            left += [n * ' '] * (q - p)\n",
    "        elif q < p:\n",
    "            right += [m * ' '] * (p - q)\n",
    "        zipped_lines = zip(left, right)\n",
    "        lines = [first_line, second_line] + [a + u * ' ' + b for a, b in zipped_lines]\n",
    "        return lines, n + m + u, max(p, q) + 2, n + u // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BinaryTree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-69f0f1cd6f22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcurr_tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m \u001b[0mcurr_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mca\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat_idx_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[0mcurr_tree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"consonant_tree\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-104-69f0f1cd6f22>\u001b[0m in \u001b[0;36mbuild_tree\u001b[1;34m(phones, allowable_atts, feat_idx_dict, use_indices, is_first)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0muse_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mcurr_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBinaryTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mcurr_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBinaryTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat_idx_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbest_feat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BinaryTree' is not defined"
     ]
    }
   ],
   "source": [
    "# Check score\n",
    "def score_split(feature, phones):\n",
    "    n_tr = 0\n",
    "    n_fa = 0\n",
    "    for phone in phones:\n",
    "        if feature in phone[2:]:\n",
    "            n_tr += 1\n",
    "        else:\n",
    "            n_fa += 1\n",
    "    total = n_tr + n_fa\n",
    "    percent = n_tr / total\n",
    "    return abs(percent - 0.5)\n",
    "\n",
    "def best_split(phones, allowable_atts):\n",
    "    atts_list = []\n",
    "    atts_scores = []\n",
    "#     print(\"phones: {}\".format(str(phones)))\n",
    "#     print(\"all atts: {}\".format(str(all_attributes)))\n",
    "#     print(\"selected atts: {}\".format(str(selected_atts)))\n",
    "    for attribute in allowable_atts:\n",
    "        score = score_split(attribute, phones)\n",
    "        atts_list.append(attribute)\n",
    "        atts_scores.append(score)\n",
    "    best_idx = np.argmin(atts_scores)\n",
    "    return atts_list[best_idx]\n",
    "\n",
    "def do_split(feature, phones, allowable_atts):\n",
    "    true_phones = []\n",
    "    false_phones = []\n",
    "    true_attributes = set()\n",
    "    false_attributes = set()\n",
    "    for phone in phones:\n",
    "        if feature in phone:\n",
    "            true_phones.append(phone)\n",
    "            for attribute in phone[2:]:\n",
    "                if attribute in allowable_atts:\n",
    "                    true_attributes.add(attribute)\n",
    "        else:\n",
    "            false_phones.append(phone)\n",
    "            for attribute in phone[2:]:\n",
    "                if attribute in allowable_atts:\n",
    "                    false_attributes.add(attribute)\n",
    "    return true_phones, false_phones, true_attributes, false_attributes    \n",
    "    \n",
    "\n",
    "\n",
    "# True values (> 0.5 are on the left, false values on the right)\n",
    "def build_tree(phones, allowable_atts, feat_idx_dict, use_indices=True, is_first=False):\n",
    "\n",
    "    # Find best feature to split on. Update binary tree\n",
    "    best_feat = best_split(phones, allowable_atts)\n",
    "    \n",
    "    if not use_indices:\n",
    "        curr_tree = BinaryTree(best_feat)\n",
    "    else:\n",
    "        curr_tree = BinaryTree(feat_idx_dict[best_feat])\n",
    "    allowable_atts.remove(best_feat)\n",
    "    tr_phones, fa_phones, tr_atts, fa_atts = do_split(best_feat, phones, allowable_atts)\n",
    "    # Update leaves of tree if possible\n",
    "    if len(tr_phones) == 1:\n",
    "        # Convert e.g. ['b consonant voiced bilabial stop labial'] to \"b\"\n",
    "        if not use_indices:\n",
    "            tr_phone = tr_phones[0][1]\n",
    "        else:\n",
    "            tr_phone = tr_phones[0][0]\n",
    "        curr_tree.left = BinaryTree(tr_phone)\n",
    "    else:\n",
    "        curr_tree.left = build_tree(tr_phones, tr_atts, feat_idx_dict, use_indices)\n",
    "        \n",
    "        \n",
    "    if len(fa_phones) == 1:\n",
    "        if not use_indices:\n",
    "            fa_phone = fa_phones[0][1]\n",
    "        else:\n",
    "            fa_phone = fa_phones[0][0]\n",
    "        curr_tree.right = BinaryTree(fa_phone)\n",
    "    else:\n",
    "        curr_tree.right = build_tree(fa_phones, fa_atts, feat_idx_dict, use_indices)\n",
    "    return curr_tree\n",
    "    \n",
    "curr_tree = build_tree(cp, ca, feat_idx_dict, use_indices=False, is_first=True)\n",
    "curr_tree.save(\"consonant_tree\")\n",
    "\n",
    "vowel_tree = build_tree(vp, va, feat_idx_dict, use_indices=False, is_first=True)\n",
    "vowel_tree.save(\"vowel_tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6 vowel near-open central\n",
    "8 vowel close-mid central rounded\n",
    "U\\ vowel near-close central rounded\n",
    "3\\ vowel open-mid central rounded\n",
    "} vowel close central rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_replicated = torch.repeat_interleave(out, repeats=num_phones, dim=0)\n",
    "mapping_t = torch.from_numpy(mapping).float().to(device)\n",
    "mapping_t = mapping_t.repeat(800, 1)\n",
    "dists = F.pairwise_distance(out_replicated, mapping_t)\n",
    "cosines = F.cosine_similarity(out_replicated, mapping_t)\n",
    "cosines = cosines.view(n, num_phones)\n",
    "dists = dists.view(n, num_phones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
